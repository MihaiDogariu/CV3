{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Se dorește scrierea unei aplicații de image retrieval care găsește cea mai similară imagine cu o imagine dată (query) dintr-o bază de date. În acest sens, trebuie construit un autoencoder care va învăța particularitățile bazei de date de antrenare. După aceea, pornind de la imaginea query, se va căuta în baza de date de test imaginea cu descriptorul de trăsături cel mai asemănător. Descriptorul va fi ales ca fiind vectorul latent/bottleneck al autoencoder-ului, iar metoda de comparație între descriptori este distanța Euclidiană. Succint, pașii ce trebuie urmați sunt:\n",
        "- Antrenarea unui autoencoder\n",
        "- Alegerea unei imagini aleatoare din baza de date de test\n",
        "- Extragerea descriptorului de trăsături (vectorul latent obținut cu autoencoder) al imaginii query;\n",
        "- Comparația descriptorului query cu toți ceilalți descriptori existenți în baza de date de test;\n",
        "- Afișarea perechii de imagini cu caracteristicile cele mai asemănătoare.\n",
        "\n",
        "\n",
        "### TODO: ###\n",
        "1. (10p) Completarea arhitecturii autoencoderului cu cate un strat suplimentar pe ultima poziție a codorului, respectiv prima poziție a decodorului. Acest strat va fi de convoluție/deconvoluție și va urmări același tipar cu restul straturilor: pentru codor, va înjumătăți numărul de canale, iar pentru decodor va dubla numărul de canale.\n",
        "2. (10p) Completarea clasei Autoencoder cu codul necesar interacțiunii dintre codor și decodor;\n",
        "3. (10p) Extragerea descriptorilor de trăsături și stocarea lor pentru toate imaginile din baza de date de test;\n",
        "4. (10p) Scrierea funcției de calcul a distanței Euclidiene;\n",
        "5. (10p) Afișarea în paralel a imaginii query cu top-3 cele mai asemănătoare imagini din baza de date de test."
      ],
      "metadata": {
        "id": "pf0xbqhjUAfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4naEx2cJsF8H"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "latent_dims = 10\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "variational_beta = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarcarea bazei de date MNIST Digits\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "img_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "FGkEKpm-xekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hiperparametri\n",
        "latent_dims = 10\n",
        "num_epochs = 3\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-2\n",
        "use_gpu = True"
      ],
      "metadata": {
        "id": "l-8b8jX029SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
        "        # de adaugat self.conv3\n",
        "        self.fc = nn.Linear(in_features=c*4*4*4, out_features=latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*4*4*4)\n",
        "        # de adaugat self.conv3\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*4, 4, 4) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n",
        "        return x\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "    # de adaugat restul functionalitatii autoencoder-ului\n",
        "\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "autoencoder = autoencoder.to(device)"
      ],
      "metadata": {
        "id": "l8CxPcyKz-Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functie de afisare a unei singure imagini\n",
        "def display(img, img_size=28, title=\"\"):\n",
        "    plt.imshow(img.cpu().detach().numpy().reshape((img_size, img_size)), cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KtMOaCGZ2fF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# setare in modul de antrenare\n",
        "autoencoder.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "\n",
        "    for image_batch, _ in train_dataloader:\n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # autoencoder reconstruction\n",
        "        image_batch_recon = autoencoder(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        reconstruction_loss = loss(image_batch_recon, image_batch)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction_loss.backward()\n",
        "\n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_avg[-1] += reconstruction_loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoca [%d / %d] Eroare de reconstructie medie: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "metadata": {
        "id": "o-lHOYBe2vx2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}