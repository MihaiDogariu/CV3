{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM3vJ0q6FG6KCbtk0Zletq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaiDogariu/CV3/blob/main/scripts/Unit_5_Datasets_and_Dataloaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "OxV-GeAyiFyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True)"
      ],
      "metadata": {
        "id": "2_JOUrF1iIbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train and test sets such that validation is the same size as test\n",
        "# and it consists of elements extracted from the train set\n",
        "val_size = len(test_dataset.targets)\n",
        "train_size = len(train_dataset.targets) - val_size\n",
        "\n",
        "train_data, val_data = torch.utils.data.random_split(\n",
        "    train_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "train_dataset = train_data.dataset\n",
        "val_dataset = val_data.dataset"
      ],
      "metadata": {
        "id": "ZKF-iZPnmFJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every custom Dataset must inherit torch.utils.data.Dataset and implement 3 functions:\n",
        "# __init__()\n",
        "# __len_()\n",
        "# __getitem__()\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "iWJsIxjvl8hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom datasets\n",
        "train_dataset = CustomDataset(train_dataset.data, train_dataset.targets)\n",
        "val_dataset = CustomDataset(val_dataset.data, val_dataset.targets)\n",
        "test_dataset = CustomDataset(test_dataset.data, test_dataset.targets)"
      ],
      "metadata": {
        "id": "MaQUzplNl-XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders based on the custom datasets\n",
        "batch_size = 67\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "AsAVwcmCyQFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all batches in the train dataloader and print some info about them\n",
        "for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    print('Batch idx:', batch_idx)\n",
        "    print(data.shape)\n",
        "    print(labels.shape)"
      ],
      "metadata": {
        "id": "XYuvwXA3y9UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a batch of images\n",
        "data, labels = next(val_loader.__iter__())\n",
        "\n",
        "num_rows = math.ceil(math.sqrt(batch_size))\n",
        "num_cols = math.ceil(batch_size / num_rows)\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
        "i = 0\n",
        "for i in range(batch_size):\n",
        "    # print(i)\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "    ax = axes[row_idx, col_idx]\n",
        "    image = data.numpy().squeeze()  # Remove the channel dimension\n",
        "    ax.imshow(image[i], cmap='gray')  # Assuming grayscale images\n",
        "    ax.set_title(f'Label: {labels[i].item()}')\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OduzBX4H0dI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}